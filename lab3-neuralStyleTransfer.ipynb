{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12stM9FrQm0dssxGW11PTB3b96aZ61VQm","timestamp":1707221666911}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","Queen Mary University of London, School of Electronic Engineering and Computer Science.\n","\n","**Author**: original notebook by Raluca Gaina, updates by Simon Colton\n","\n","ECS7022P: Computational Creativity\n","===\n","\n","---\n","\n","# **Practical 3. Implementing Neural Style Transfer**\n","\n","In this practical you will implement a method to take a *content image* and activate it to gain some of the visual look of a *style image*. To do this, we'll think of the RGB values in the image as being trainable, and define a loss function which drives the training. The loss function will be based on a pre-trained machine vision model that you will download. There will be two terms to the loss function as per lecture 3: one term using the output of the early layers of the pre-trained model (to gain the fine detail of the style image) and another term using the output of the final layer of the pre-trained model (to preserve the content of the content image).\n","\n","Please turn on \"Menu\" => \"View => \"Table of Contents\" to navigate this notebook.\n"],"metadata":{"id":"fieAulOaeJr5"}},{"cell_type":"markdown","source":["## TODO 1 - Copy the Notebook and Enable GPU Runtime\n","\n","Create your own copy of this Colab notebook, using \"Menu\" => \"File\" => \"Save a Copy In Drive\" and then opening that copy. You should now be able to edit that version and save your changes.\n","\n","Make sure that you enable GPU for the session, using \"Menu\" => \"Runtime\" => \"Change Runtime Type\".\n","\n","**Important**:  \n","When you have finished the practical, make sure you delete and disconnect the runtime, otherwise you may have restricted GPU access next time you use Colab notebooks.\n"],"metadata":{"id":"SPZwFOpxfkZU"}},{"cell_type":"markdown","source":["# Setting Up"],"metadata":{"id":"GefQnuVghbvh"}},{"cell_type":"code","source":["#@title TODO 2 - Imports and Checking GPUs\n","\n","#@markdown View the code in this cell to see which imports are required for the\n","#@markdown notebook. We are using TensorFlow, a well known deep learning development\n","#@markdown kit. See how we've used tensorflow to check whether a GPU has been assigned.\n","#@markdown If it hasn't found one, try connecting to a GPU via the runtime menu,\n","#@markdown and then running this cell again. To keep things tidy, once run, clear\n","#@markdown the output from this cell\n","\n","#@markdown Search online for the tensorflow.keras.applications package - what\n","#@markdown other pre-trained models does it have available?\n","\n","import tensorflow as tf\n","import keras\n","import numpy as np\n","from tensorflow.keras.utils import get_file\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import applications\n","from tensorflow.keras.utils import plot_model\n","from keras import Model\n","from tensorflow.keras.optimizers import SGD\n","from IPython import display\n","import os\n","\n","device_name = tf.test.gpu_device_name()\n","print('Found GPU at: {}'.format(device_name))\n","print(\"TensorFlow version:\", tf.__version__)"],"metadata":{"id":"J215pnm1fyUN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707221804082,"user_tz":0,"elapsed":3633,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"71c608b8-d29f-434a-9b07-d5b1f7343622"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n","TensorFlow version: 2.15.0\n"]}]},{"cell_type":"markdown","source":["## TODO 3 - Downloading the Machine Vision Model\n","\n","In this practical, we're using a pre-trained model, as is typical for such applications. Therefore, for our network definition we simply load a suitable model. The model used has been shown in [some tests](https://towardsdatascience.com/practical-techniques-for-getting-style-transfer-to-work-19884a0d69eb) to influence the resulting images, so this is one point to experiment with when looking for particular outcomes.\n","\n","Here, we'll use the [VGG19 model](https://iq.opengenus.org/vgg19-architecture/#:~:text=VGG19%20is%20a%20variant%20of,VGG19%20has%2019.6%20billion%20FLOPs.) provided with Keras. VGG16 is also an option compatible with the notebook - other models would require some more adjustments in setting up the model and image preprocessing.\n","\n","Look at the code in the cell below to see how we use Keras applications to download the pre-trained model.\n","\n","---\n","\n","**Question**: Why is it called VGG19?\n","\n","**Answer**:\n","\n","VGG = (V)isual (G)eometry (G)roup (at the University of Oxford)\n","\n","19 refers to the version of VGG having 19 layers (16 convolutional + 3 fully connected)."],"metadata":{"id":"SioKkR9egjfC"}},{"cell_type":"code","source":["network_choice = \"vgg19\" #@param['vgg19', 'vgg16']\n","\n","# Note that there is a network object and a model object (they're different!)\n","\n","network_dict = {\n","    \"vgg19\": applications.vgg19,\n","    \"vgg16\": applications.vgg16,\n","}\n","network = network_dict[network_choice]\n","\n","model_dict = {\n","    \"vgg19\": applications.vgg19.VGG19(weights=\"imagenet\", include_top=False),\n","    \"vgg16\": applications.vgg16.VGG16(weights=\"imagenet\", include_top=False),\n","}\n","\n","# We will call this later when needed\n","def build_model():\n","    model = model_dict[network_choice]\n","    model.summary()\n","    return model\n"],"metadata":{"id":"CK8qhLMnhi54","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707223169036,"user_tz":0,"elapsed":2019,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"e656fceb-be66-4812-9002-0cd9da49af75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80134624/80134624 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["#@title ## TODO 4 - Set up a Function to View Images\n","\n","#@markdown There is one last thing to set up, which is to define a function which can\n","#@markdown be used to view our content, style and style-transferred image.\n","#@markdown Currently, this function is not finished.\n","#@markdown Cut and paste the relevant code to finish this function off, by enabling it\n","#@markdown to show the style transfer image as well as the content and style image\n","\n","\n","# Displays content, style and combination images, labelled. Any could be missing if set to None\n","def display_images(content_image=None, style_image=None, style_transfer_image=None):\n","    # For each image, displaying in the subplot if not `None`\n","    _, cells = plt.subplots(1, 3, figsize=(45,15))\n","    if content_image is not None:\n","        cells[0].imshow(content_image)\n","        cells[0].set_title(\"Content\", fontsize=30)\n","    if style_image is not None:\n","        cells[1].imshow(style_image)\n","        cells[1].set_title(\"Style\", fontsize=30)\n","    # TODO 4.1 add style transfer image to the cells\n","    if style_transfer_image is not None:\n","        cells[2].imshow(style_transfer_image)\n","        cells[2].set_title(\"Style transfer\", fontsize=30)\n","    for cell in cells:\n","        cell.axis('off')\n","    plt.show()"],"metadata":{"id":"KsuhDGKSjW-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading and Preparing the Image Data"],"metadata":{"id":"Hpx4zCRqkKuV"}},{"cell_type":"code","source":["#@title # TODO 5 - Download Images\n","#@markdown Add code to this cell which uses the `keras.utils.get_file` and `load_img` function to download these\n","#@markdown images and load them into keras images with variable names `style_image` and `content_image`.\n","\n","#@markdown Your code should also use keras.utils.load_img to load the image.\n","#@markdown Choose other image URLs if you have things you want to try out!\n","\n","style_image_url = \"https://i.imgur.com/9ooB60I.jpg\" # @param {type: \"string\"}\n","content_image_url = \"https://tourism.euskadi.eus/contenidos/d_destinos_turisticos/0000004981_d2_rec_turismo/en_4981/images/CT_cabecerabilbaoguggen.jpg\" # @param {type: \"string\"}\n","\n","# TODO 5.1\n","style_image_path = keras.utils.get_file(origin=style_image_url)\n","content_image_path = keras.utils.get_file(origin=content_image_url)\n","\n","# TODO 5.2\n","style_image = keras.utils.load_img(style_image_path)\n","content_image = keras.utils.load_img(content_image_path)"],"metadata":{"id":"liKOHVR6k2Ll","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707226701733,"user_tz":0,"elapsed":4716,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"020d92fb-367f-491a-ec1f-ce94506c399a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://tourism.euskadi.eus/contenidos/d_destinos_turisticos/0000004981_d2_rec_turismo/en_4981/images/CT_cabecerabilbaoguggen.jpg\n","201498/201498 [==============================] - 2s 10us/step\n"]}]},{"cell_type":"code","source":["#@title # TODO 6 - Use this cell to view the images you have downloaded\n","\n","#@markdown Click the images to see them in detail.\n","\n","display_images(content_image, style_image)"],"metadata":{"id":"nqAmCxHknAbu","colab":{"base_uri":"https://localhost:8080/","height":390,"output_embedded_package_id":"1O22z8hFGCXlp5Epir7qYhiDQndO-bjTF"},"executionInfo":{"status":"ok","timestamp":1707226707969,"user_tz":0,"elapsed":6247,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"30856f6d-ab87-4d45-c530-ac0bf67691ae"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["#@title # TODO 7 - Extract and Scale the Image Data\n","\n","#@markdown Look in this cell to see how the image data is extracted.\n","#@markdown Choose a height (img_height) for your stylised image\n","\n","# Set the img_height variable\n","img_height = 400 #@param{type: 'number'}\n","\n","# Turn images into Numpy arrays to find dimensions\n","content_image_np = keras.utils.img_to_array(content_image)\n","style_image_np = keras.utils.img_to_array(style_image)\n","\n","print(f\"The content image shape is: {content_image_np.shape}\")\n","print(f\"The style image shape is: {style_image_np.shape}\")\n","\n","# Setting image dimensions variables\n","height = content_image_np.shape[0]\n","width = content_image_np.shape[1]\n","channels = content_image_np.shape[2]\n","\n","# Scale the width appropriately to get the img_width variable\n","img_width = int(width * (img_height/height))\n","\n","# Set the img_size variable (used later)\n","img_size = img_height * img_width"],"metadata":{"id":"VmFKV7N-ofCq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707226707970,"user_tz":0,"elapsed":13,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"5459cf2f-15af-406b-ee0a-8c60a12b6b48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The content image shape is: (1200, 1920, 3)\n","The style image shape is: (934, 1160, 3)\n"]}]},{"cell_type":"markdown","source":["# Implementing Image Handling Functions\n","\n","We need to preprocess the images to be compatible with the expected input for the network used. In our case (VGG19/VGG16 models), we need a tensor representing a batch of images (even if batch size is 1 when processing images individually). Importantly, images also need to be pre-processed by the corresponding model using the Keras `preprocess_input(img)` function.\n","\n","As the network outputs tensors, we'll also need a utility function to turn tensors back into Numpy array images in order to display the results. To reverse the `preprocess_input(img)` function, we add the mean values for each channel of the Imagenet dataset (found [here](https://github.com/DeepVoltaire/AutoAugment/issues/4)) and transform the image from BGR to RGB.\n","\n","Examine the code below to see how we process an image for the model, and then translate the output back to an image."],"metadata":{"id":"4hRfHm4rqX2n"}},{"cell_type":"code","source":["#@title # TODO 8 - Answer Questions in the Code\n","# Open, resize and format pictures into appropriate tensors\n","def preprocess_image(image_path):\n","    # Load image with given size into PIL format\n","    img = keras.utils.load_img(image_path, target_size=(img_height,img_width))\n","\n","    # Turn image into Numpy array\n","    img = keras.utils.img_to_array(img)\n","\n","    # The next step expects a batch of images, so add another dimension\n","    img = np.expand_dims(img, axis=0)\n","\n","    # Call the network preprocessing step.\n","    # This converts to BGR and zero-centers data with regards to the Imagenet dataset,\n","    # by subtracting the mean channel values in Imagenet\n","    img = network.preprocess_input(img)\n","\n","    # Transform the img to a tensor and return this\n","    return tf.convert_to_tensor(img)\n","\n","\n","# Reverse the preprocessing step to turn the tensor into an RGB Numpy array which we can visualise\n","def deprocess_image(tensor):\n","    # Transform the tensor into a Numpy array\n","    img = tensor.numpy()[0]\n","\n","    # Return BGR values to normal (non-zero-centered), adding back in the means of the Imagenet dataset\n","\n","    # TODO 8.1 - what does the [:, :, 0] notation below mean?\n","    '''\n","    Answer:\n","    [:, :, 0] refers to the 1st element (i.e. at index 0) of each 3rd\n","    dimensional array, i.e. we select all 1st dimensional arrays, all 2nd\n","    dimensional arrays and the 1st index's value of the 3rd dimensional arrays.\n","    For example, [:, :, 0] for the array...\n","    [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n","    ... becomes...\n","    [[[1], [3]], [[5], [7]]]\n","\n","    NOTE:\n","    In our case, the 3rd dimension consists of arrays size 3 and refers to:\n","    - R (red, at index = 0)\n","    - G (green, at index = 1)\n","    - B (blue, at index = 2)\n","    '''\n","    img[:, :, 0] += 103.939\n","    img[:, :, 1] += 116.779\n","    img[:, :, 2] += 123.68\n","\n","    # Convert from BGR to RGB\n","    # TODO 8.2 - what does the [:, :, ::-1] notation mean?\n","    '''\n","    Answer:\n","    [:, :, ::-1] reverses the order of the elements in the 3rd dimension, i.e.\n","    every 3rd dimensional array becomes reversed (each, not together).\n","    For example...\n","    [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n","    ... becomes...\n","    [[[2, 1], [4, 3]], [[6, 5], [8, 7]]]\n","    '''\n","    img = img[:, :, ::-1]\n","\n","    # Ensure values are in valid ranges\n","    # TODO 8.3 - what does the clip function do?\n","    '''\n","    Answer:\n","    `numpy.clip` clips, i.e. limits the values in an array. Given an interval,\n","    values outside the interval are clipped to the interval edges. For example,\n","    if an interval of [0, 1] is specified, values smaller than 0 become 0, and\n","    values larger than 1 become 1.\n","\n","    SIDE NOTE:\n","    Equivalent to but faster than `np.minimum(a_max, np.maximum(a, a_min))`,\n","    given a xlipping, i.e. limiting interval [a_min, a_max].\n","\n","    REFERENCE:\n","    https://numpy.org/doc/stable/reference/generated/numpy.clip.html\n","    '''\n","\n","    img = np.clip(img, 0, 255).astype(\"uint8\")\n","    # Here, we are limiting the values to between 0 and 255 (inclusive).\n","\n","    return img"],"metadata":{"id":"O9xrCmbFrgqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**DEMO**: A small demo of the reversing technique used above:"],"metadata":{"id":"ipsmaVPsfSQv"}},{"cell_type":"code","source":["a = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n","print(\"Unreversed:\\n\", a)\n","print(\"\\nReversed in 1st dimension:\\n\", a[::-1, :, :])\n","print(\"\\nReversed in 2nd dimension:\\n\", a[:, ::-1, :])\n","print(\"\\nReversed in 3rd dimension:\\n\", a[:, :, ::-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilc1_8jadhPu","executionInfo":{"status":"ok","timestamp":1707226707971,"user_tz":0,"elapsed":11,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"a30c0921-2071-4ef7-c6c0-615eb2f5f4e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unreversed:\n"," [[[1 2]\n","  [3 4]]\n","\n"," [[5 6]\n","  [7 8]]]\n","\n","Reversed in 1st dimension:\n"," [[[5 6]\n","  [7 8]]\n","\n"," [[1 2]\n","  [3 4]]]\n","\n","Reversed in 2nd dimension:\n"," [[[3 4]\n","  [1 2]]\n","\n"," [[7 8]\n","  [5 6]]]\n","\n","Reversed in 3rd dimension:\n"," [[[2 1]\n","  [4 3]]\n","\n"," [[6 5]\n","  [8 7]]]\n"]}]},{"cell_type":"code","source":["#@title # TODO 9 - Implement Code to test the Encoding/Decoding of Images\n","\n","#@markdown Fill out the code in this cell to check that your pre_processing and deprocessing\n","#@markdown functions work.\n","\n","tensor = preprocess_image(style_image_path)\n","print(tensor.shape)\n","img = deprocess_image(tensor)\n","display_images(style_image=img)"],"metadata":{"id":"Z_uLsOa3uHJj","colab":{"base_uri":"https://localhost:8080/","height":407,"output_embedded_package_id":"1kzcYDB9wwWBNF7dDmxrpBxm7afZI7S85"},"executionInfo":{"status":"ok","timestamp":1707226710358,"user_tz":0,"elapsed":2396,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"4b160fbc-32c5-49ee-de09-326cfa37ce6c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Defining the Training Configuration\n","\n","To get everything set up, we first need to build the VGG model, then define a new, more granular, model which contains information (input/output) for each of the layers. We'll use this to extract features from different layers when computing the loss for the image combination.\n","\n","To combine the 2 images, we'll repeat 2 steps for several iterations: calculate the loss and gradients of the current combined image, then use the optimizer to apply the gradients and modify the combined image towards our desired output."],"metadata":{"id":"r8VF4o7sx1C7"}},{"cell_type":"code","source":["#@title # Setting up the New Model\n","\n","model = build_model()\n","\n","# Set up a model to extract features, given input, for each layer in the network\n","outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n","feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict)\n"],"metadata":{"id":"8_XQT1UBx-dP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707226710359,"user_tz":0,"elapsed":34,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"fb6bf13c-648c-4b51-c0de-5129ab8bf37f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"vgg19\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None, None, 3)]   0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n","                                                                 \n"," block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n","                                                                 \n"," block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n","                                                                 \n"," block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n","                                                                 \n","=================================================================\n","Total params: 20024384 (76.39 MB)\n","Trainable params: 20024384 (76.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["#@title # Define the Loss Function\n","\n","#@markdown We need to define a custom loss function that achieves the desired\n","#@markdown combination effect of the 2 input images. In the combination image,\n","#@markdown the content is given by the feature values in the intermediate layers.\n","#@markdown The style, however, is given by means and correlations of different\n","#@markdown feature maps. This can be calculated with a Gram matrix, given the\n","#@markdown features in a layer.\n","\n","#@markdown This is the most important cell in the notebook. Please go through each defined function\n","#@markdown and look up each of the TensorFlow functions that are used (in the TensorFlow\n","#@markdown manual web pages). Make sure you understand each of the functions, including:\n","#@markdown `transpose`, `reshape`, `matmul`, `reduce_sum`, `concat`, `zeros` and `GradientTape`.\n","\n","#@markdown Remember that `feature_extractor` was defined above.\n","#@markdown Check the definition for the gram matrix against the lecture notes.\n","\n","#@title Loss function\n","\n","# Calculate the Gram matrix for given input\n","def gram_matrix(input_tensor):\n","  input_tensor = tf.transpose(input_tensor, (2, 0, 1))  # Transpose\n","  features = tf.reshape(input_tensor, (tf.shape(input_tensor)[0], -1))  # Flatten layer\n","  gram_matrix = tf.matmul(features, tf.transpose(features))\n","  return gram_matrix\n","\n","# The style loss is the mean square error between the Gram matrix of the style\n","# image features and the Gram matrix of the combination image features,\n","# divided by 4 in original paper (see link at the end of the notebook)\n","def style_loss(style, combination):\n","  S = gram_matrix(style)\n","  C = gram_matrix(combination)\n","  return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (img_size ** 2))\n","\n","# The content loss is the mean square error between the combination and the content image features\n","def content_loss(content, combination):\n","    return tf.reduce_sum(tf.square(combination - content))\n","\n","# Calculate the loss function given a content image, a style image and the combination result\n","def loss_function(combination_image, content_image, style_image, content_layers, style_layers):\n","\n","    # Combine all the images in the same tensor\n","    input_tensor = tf.concat([content_image, style_image, combination_image], axis=0)\n","\n","    # Get the features in all the layers for the three images\n","    features = feature_extractor(input_tensor)\n","\n","    # Initialise the loss\n","    loss = tf.zeros(shape=())\n","\n","    # Extract the content layers and calculate the content loss\n","    for layer_name in content_layers:\n","        layer_features = features[layer_name]\n","        content_image_features = layer_features[0, :, :, :]  # Content image is at position 0\n","        combination_image_features = layer_features[2, :, :, :]  # Combination image is at position 2\n","        loss += content_weight * content_loss(content_image_features, combination_image_features)\n","\n","    # Extract the style layers and calculate the style loss\n","    for layer_name in style_layers:\n","        layer_features = features[layer_name]\n","        style_image_features = layer_features[1, :, :, :]  # Style image is at position 1\n","        combination_image_features = layer_features[2, :, :, :]\n","        loss += style_weight * style_loss(style_image_features, combination_image_features)\n","\n","    return loss\n","\n","# This brings together the calculation of loss and that of gradients\n","def compute_loss_and_grads(content_image, style_image, combination_image, content_layers, style_layers):\n","    with tf.GradientTape() as tape:\n","        loss = loss_function(combination_image, content_image, style_image, content_layers, style_layers)\n","        grads = tape.gradient(loss, combination_image)\n","    return loss, grads"],"metadata":{"id":"KqiwKL7pYmlI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title # TODO 10 - Setting the Weights for the Loss Function\n","\n","#@markdown Have a look at the style and content layers that we've chosen and see where\n","#@markdown they are in the output model description from the cell above.\n","\n","#@markdown Note that we need to compile the optimizer, which is done at the bottom of the cell.\n","# Total loss is a weighted sum of content and style loss\n","content_weight = 2.5e-8  #@param\n","style_weight = 1.2e-6  #@param default is 1e-6\n","\n","# Set which layers of the network should be taken into consideration when calculating content and style loss\n","style_layers = [\n","    \"block1_conv1\",\n","    \"block2_conv1\",\n","    \"block3_conv1\",\n","    \"block4_conv1\",\n","    \"block5_conv1\",\n","]\n","content_layers = [\"block5_conv2\"]\n","\n","content_weight /= len(content_layers)\n","style_weight /= len(style_layers)\n"],"metadata":{"id":"lEXbhzs4Z6VK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title # TODO 11 - Define and compile the Optimizer\n","\n","#@markdown Using the `#@param` keyword, add in three GUI fields to allow the user to define\n","#@markdown three variables: `initial_learning_rate`, `decay_steps` and `decay_rate`.\n","#@markdown The default values for these should be `100.0`, `100`, and `0.96` respectively.\n","#@markdown Code at the bottom of the cell uses these values to define the optimizer.\n","\n","#@markdown Also, look up the SGD optimizer [here](https://keras.io/api/optimizers/sgd/) and\n","#@markdown check out the other parameters which could be specified.\n","\n","#@markdown Note that we have to compile the optimizer, as this does not happen\n","#@markdown when the object is instantiated.\n","\n","# TODO 11.1\n","# Parameters for training\n","initial_learning_rate = 100.0 # @param {type:\"number\"}\n","decay_steps  = 100.0 # @param {type:\"number\"}\n","decay_rate = 0.96 # @param {type:\"number\"}\n","\n","optimizer = SGD(tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_learning_rate, decay_steps=decay_steps, decay_rate=decay_rate))\n","\n","# We can now compile (i.e., set up) the optimizer.\n","model.compile(optimizer, loss_function)"],"metadata":{"id":"TuFzV8QxXYo-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title # TODO 12 - Saving Intermediate Models and Images\n","#@markdown In the code in this cell, add in some default values for whether to\n","#@markdown save the image and/or the model.\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Directory where the checkpoints will be saved\n","path = '/content/gdrive/My Drive/Work/Colab/StyleTransfer/' #@param{type: 'string'}\n","\n","# TODO 12.1 - Add in default values to this function so that the model isn't saved by default, but the image is.\n","def save(epoch, img_tensor, save_image, save_model):\n","  model_name = path + \"model_\" + str(epoch) + '.tf'\n","  image_name = path + \"img_\" + str(epoch) + '.png'\n","\n","  # Save image\n","  if save_image:\n","      img = deprocess_image(img_tensor)\n","      keras.utils.save_img(image_name, img)\n","\n","  # Save model\n","  if save_model:\n","      tf.saved_model.save(model, model_name)\n","\n","# This ensures that the path to saving directory exists (so you don't have to create it manually)\n","os.makedirs(path, exist_ok=True)"],"metadata":{"id":"9-OTEW0ThbyS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707226712156,"user_tz":0,"elapsed":1807,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"db4832b5-5c30-413f-ab6e-347be7f63693"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Generating Stylized Images in an Optimization Loop\n","\n","The goal here is not to train the network, but to first, calculate the loss and gradients for our combination image, and second, apply the gradients to the combination image and transform it towards our desired output.\n","\n","At each iteration of these 2 steps (or every X iterations if running for longer periods of time) we can display the current state of the result with functions defined earlier, and/or save the images generated and models.\n","\n","The initial input to the network is going to be 3 tensors: the content image (preprocessed), the syle image (preprocessed) and the combination image (a tensorflow variable, initialised to the values of the content image to speed up the process).\n","\n","**Implement** this optimization process, following these steps:\n","\n","1. Create a for or while loop, which should stop after the given number of **iterations**.\n","2. Within this loop, add the **first step** of the iteration, using functions defined above: calculate the loss and the gradients, given the 3 input tensors.\n","3. Within the for loop, add the **second step** of the iteration, using the optimizer's `apply_gradients` function. The input to this function is a list of size 1 of tuples containing the gradients and the combination image tensor (which needs to be modified appropriately).\n","4. **Print** out the current iteration number and loss.\n","5. Use the function defined earlier to **display** the 3 images (content, style, deprocessed combination image) every 10 or so iterations (this is quite an expensive operation, so it'd take longer if you did it at each iteration).\n","6. Optionally, **save** the current combination image and/or model using the saving function above.\n","\n","With everything implemented, run this cell and watch the image transform! (If you want to work from the solutions, feel free to check out the answer below). Don't forget to click the images to see them in detail (smaller image sizes don't usually show the fine texture detail in the stylised image).\n","\n","Try different style layer choices, as well as weightings for the two loss function parts. And also try different content and style images to see if you can get good results!"],"metadata":{"id":"JCf7ptxshNHg"}},{"cell_type":"code","source":["#@title # TODO 13 - Implement the Optimisation loop\n","\n","# Note that the optimizer has to be redefined on each run (not sure why!)\n","optimizer = SGD(tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_learning_rate, decay_steps=decay_steps, decay_rate=decay_rate))\n","\n","iterations = 100  #@param{type: 'number'}\n","\n","content_image_tensor = preprocess_image(content_image_path)\n","style_image_tensor = preprocess_image(style_image_path)\n","combination_image_tensor = tf.Variable(preprocess_image(content_image_path))\n","\n","# TODO 13 Implement the optimisation routine described above\n","for iteration in range(iterations):\n","  # Step 1: calculate loss and gradients\n","  # TODO 13.1 - Add code here\n","  loss, grads = compute_loss_and_grads(content_image_tensor, style_image_tensor, combination_image_tensor, content_layers, style_layers)\n","\n","  # Step 2: apply gradients\n","  # TODO 13.2 - Add code here\n","  optimizer.apply_gradients([(grads, combination_image_tensor)])\n","\n","  # Step 3: Display images every 50 iterations and at the end\n","  if iteration % 50 == 0 or (iteration == iterations - 1):\n","    # Display the three images and print the loss\n","    # TODO 13.3 - Add code here\n","    combination_image = deprocess_image(combination_image_tensor)\n","    display_images(content_image, style_image, combination_image)\n","    print(\"Loss:\", float(loss))\n","    # NOTE: `content_image` and `style_images` were defined previously\n","\n","    # Save image and/or model\n","    # TODO 13.4 - Add code here\n","    # Copied from solutions as I was lazy...\n","    save(iteration, combination_image_tensor, save_image=True, save_model=False)"],"metadata":{"id":"sZpCPG9Bjppz","colab":{"base_uri":"https://localhost:8080/","height":913,"output_embedded_package_id":"1ixXCjs21tGkULAYIdD3uqEDgDgrdbJ3d"},"executionInfo":{"status":"ok","timestamp":1707227397432,"user_tz":0,"elapsed":58661,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"ea97fa15-e8db-4ae7-9028-6d2e8bd4a3b2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Different Approaches\n","\n","There are other models set up for achieving similar results very quickly and without the need to define any other elements. We can simply load the `tensorflow_hub` package, choose one of the models available and pass into it a content and a style image. This will output an array containing one image, which we can display by turning the resulting tensor back into a Numpy array.\n","\n","Try this out and check the differences in speed / quality of the output."],"metadata":{"id":"ZDVtSyJGshok"}},{"cell_type":"code","source":["#@title # TODO 14 - TensorFlow Hub\n","\n","#@markdown Firstly, check out the [TensorFlow Hub](https://www.tensorflow.org/hub)\n","#@markdown where you can download lots of useful pre-trained models, including ones\n","#@markdown for style transfer and other effects, like cartoonization.\n","\n","#@markdown Then run this cell and see the results. Try changing the style and content\n","#@markdown images using the cells above, and see what you get.\n","\n","import tensorflow_hub as hub\n","hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n","\n","# We need to scale the image data to 256x256 pixels\n","# Using the numpy arrays representing our images, with 1 batch dimension added, and normalized\n","c_img = np.expand_dims(content_image_np, axis=0)/255\n","s_img = np.expand_dims(style_image_np, axis=0)/255\n","\n","stylized_images = hub_model(tf.constant(c_img), tf.constant(s_img))[0]\n","print(stylized_images.shape)\n","display_images(c_img[0], s_img[0], stylized_images[0])"],"metadata":{"id":"msFm4KKfsxJv","colab":{"base_uri":"https://localhost:8080/","height":316,"output_embedded_package_id":"1Y-GlDo6fFuzMReVvqSTfVXmnoYnE-_as"},"executionInfo":{"status":"ok","timestamp":1707226802232,"user_tz":0,"elapsed":20049,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}},"outputId":"402009b4-431b-4b35-edfc-210dec015c8b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# To Explore Further\n","\n","Here are some more style transfer apps, videos and repositories to check out and play around with!\n","\n","1. [Arbitrary Style Transfer in the Browser](https://reiinakano.com/arbitrary-image-stylization-tfjs/) is a browser application which allows to select from a range of content and style images to produce a combined image. It also includes some customization from the output, with a choice of different networks as well.\n","\n","2. [Art Done Quick Style Transfer](http://www.artdonequick.com/). Check out a video of how I (Simon Colton) added style transfer to the Art Done Quick casual creator app.\n","\n","3. [Magenta Style Transfer](https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/). The Magenta team from Google have an in-browser application which works nicely. There are also technical details of how it operates.\n","\n","4. [AI Painter](https://www.instapainting.com/ai-painter) utilises Style Transfer to customize a picture uploaded by the user into a painting of their liking, with the option of then hiring a painter which would turn the digital output into an actual painting. This is now a commercial site, but check it out to see how people are making money via style transfer.\n","\n","5. [Ostagram](https://www.ostagram.me/). Go here to see some amazingly good (curated/cherry picked) style transfer examples, and some pretty bad ones!\n"],"metadata":{"id":"eNYOKTDgsDw3"}},{"cell_type":"markdown","source":["# Solutions"],"metadata":{"id":"qPDWYvaQlKIS"}},{"cell_type":"code","source":["#@markdown Only look at these if you haven't worked out the answer for yourselves.\n","#@markdown Also, please ask for help - that's what we're here for!\n","\n","# VGG is named after the (v)isual (g)eometry (g)roup at the University of Oxford\n","# https://www.robots.ox.ac.uk/~vgg/\n","# 16 and 19 refer to the number of convolution layers in the models.\n","\n","# TODO 4.1\n","# This is the code that can be copied from what is already there.\n","\n","# if style_transfer_image is not None:\n","#    cells[2].imshow(style_transfer_image)\n","#    cells[2].set_title(\"Combination\", fontsize=30)\n","\n","# TODO 5.1\n","# content_image_path = get_file(fname = \"content.jpg\", origin = content_image_url) # TODO\n","# style_image_path = get_file(fname = \"style.jpg\", origin = style_image_url) # TODO\n","\n","# TODO 5.2\n","# content_image = keras.utils.load_img(content_image_path)\n","# style_image = keras.utils.load_img(style_image_path)\n","\n","# TODO 8.1 - what does the [:, :, 0] notation below mean?\n","# Answer: it points to the collection of all (r)ed pixel values\n","\n","# TODO 8.2 - what does the [:, :, ::-1] notation mean?\n","# Answer: it reverses BGR pixels to RGB\n","\n","# TODO 8.3 - what does the clip function do?\n","# Answer: it limits the range of the values (in this case to be between 0 and 255)\n","\n","# TODO 9.1\n","# tensor = preprocess_image(style_image_path)\n","# print(tensor.shape)\n","# img = deprocess_image(tensor)\n","# display_images(None, img, None)\n","\n","# TODO 11.1\n","# initial_learning_rate = 100.0 #@param\n","# decay_steps = 100 #@param\n","# decay_rate = 0.96 #@param\n","\n","# TODO 12.1\n","# The function definition needs to change to:\n","# def save(epoch, img_tensor, save_image=True, save_model=False):\n","\n","# TODO 13.1\n","# loss, grads = compute_loss_and_grads(content_image_tensor, style_image_tensor, combination_image_tensor, content_layers, style_layers)\n","\n","# TODO 13.2\n","# optimizer.apply_gradients([(grads, combination_image_tensor)])\n","\n","# TODO 13.3\n","# print(f\"Iteration {iteration + 1} loss={loss:.3f}\")\n","# if iteration % 10 == 0:\n","#    display_images(content_image, style_image, deprocess_image(combination_image_tensor))\n","\n","# TODO 13.4\n","# save(iteration, combination_image_tensor, save_image=True, save_model=False)"],"metadata":{"id":"-BJDMO0ckuBW","executionInfo":{"status":"ok","timestamp":1707248918549,"user_tz":0,"elapsed":8,"user":{"displayName":"Pranav Gopalkrishna","userId":"01062470237629432661"}}},"execution_count":1,"outputs":[]}]}